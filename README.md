This is a model I used to classify the data on Kaggle's Contradictory, My Dear Watson. I tried this a few ways, using BERT, BERT multilingual, and mT5 first, but getting mediocre results. Finally, I got my best results using 'mDeBERTa-v3-base-xnli-multilingual', which I found from luistalavera on kaggle below.  I got to .847 accuracy which was good for 17th on their leaderboard at the time.


https://www.kaggle.com/code/luistalavera/contradictory-my-dear-watson-eda-huggingface
